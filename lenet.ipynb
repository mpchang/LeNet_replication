{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from memory_profiler import profile \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script will train a neural network to identify numbers from images taken \n",
    "# from zip codes written on envelopes from the USPS. \n",
    "# \n",
    "# Tasks: \n",
    "# 1) Import the training and test data \n",
    "# 2) Plot a few candidate data points \n",
    "# 3) Build a convolutional neural net model to train on the test data \n",
    "# 4) Inspect the training and test losses \n",
    "# 5) Observe error rate and error rate, given a particular rejection rate. \n",
    "# 6) Observe the activation and gradient statistics of the model\n",
    "# 7) Sample the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Length = 7291, Test Data Length = 2007\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xte = [], []\n",
    "Ytr, Yte = [], []\n",
    "\n",
    "with open(\"dataset/zip.train\", 'r') as f: \n",
    "    dataset = f.read().splitlines()\n",
    "    for data in dataset: \n",
    "        data = data.split()\n",
    "        Ytr.append(int(float(data[0])))\n",
    "        Xtr.append([float(d) for d in data[1:]])\n",
    "    Ytr = torch.tensor(Ytr)\n",
    "    Xtr = torch.tensor(Xtr)\n",
    "    Xtr = Xtr.view(Xtr.shape[0], 16, 16)\n",
    "    \n",
    "with open(\"dataset/zip.test\", \"r\") as f: \n",
    "    dataset = f.read().splitlines()\n",
    "    for data in dataset: \n",
    "        data = data.split()\n",
    "        Yte.append(int(float(data[0])))\n",
    "        Xte.append([float(d) for d in data[1:]])\n",
    "    Yte = torch.tensor(Yte)\n",
    "    Xte = torch.tensor(Xte)\n",
    "    Xte = Xte.view(Xte.shape[0], 16, 16)\n",
    "\n",
    "print(f\"Training Data Length = {len(data_train)}, Test Data Length = {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples from the Training Set\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Examples from the Training Set\")\n",
    "fig, axs = plt.subplots(3,3)\n",
    "ix = torch.randint(0, Xtr.shape[0], (9, ))\n",
    "i = 0\n",
    "for row in axs: \n",
    "    for col in row: \n",
    "        col.imshow(Xtr[ix[i]])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create Layer, BatchNorm, Tanh classes\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "class Linear: \n",
    "    def __init__(self, fanin, fanout, bias=False):\n",
    "        self.weight = torch.randn([fanin, fanout]) / (fanin**0.5)\n",
    "        self.bias = torch.zeros(fanout) if bias else None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear({tuple(self.weight.shape)})\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None: \n",
    "            return self.out + self.bias\n",
    "        else: \n",
    "            return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "class BatchNorm1d: \n",
    "    def __init__(self, dim, momentum=0.1, tol=1e-5, training=True):\n",
    "        # dim is the number of examples used in the batch \n",
    "        self.momentum = momentum \n",
    "        self.training = training\n",
    "        self.dim = dim \n",
    "        self.tol = tol\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BatchNorm1d[({self.dim}, {self.momentum}) | {self.training}]\"\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        if self.training: \n",
    "            if x.ndim == 2: \n",
    "                dim = 0\n",
    "            elif x.ndim == 3: \n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdims=True)\n",
    "            xvar = x.var(dim, keepdims=True, correction=1) # bessel's correction \n",
    "            self.out = self.gamma * (x - xmean) * (xvar + self.tol) ** (-0.5) + self.beta\n",
    "            with torch.no_grad(): \n",
    "                self.running_mean = self.momentum * xmean + (1-self.momentum) * self.running_mean\n",
    "                self.running_var = self.momentum * xvar + (1-self.momentum) * self.running_var\n",
    "            \n",
    "        else: \n",
    "            self.out = self.gamma * (x - self.running_mean) * (self.running_var + self.tol) ** (-0.5) + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "    def __repr__(self):\n",
    "        return f\"Tanh\"\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "class Conv2d: \n",
    "    def __init__(self, channels_in, channels_out, kernel_size, stride, padding, bias=True): \n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        if isinstance(self.padding, int): \n",
    "            self.padding = (padding, padding)   # symmetrical padding is required \n",
    "        self.stride = stride\n",
    "        if isinstance(self.stride, int): \n",
    "            self.stride = (stride, stride)\n",
    "        if isinstance(self.kernel_size, int): \n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.weight = torch.randn(channels_out, self.kernel_size[0] * self.kernel_size[1] * channels_in)\n",
    "        self.bias = torch.randn(channels_out, 1, 1) if bias else None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Conv2d({self.channels_in}ch_in, {self.channels_out}ch_out, {self.kernel_size} kernel_size, {self.stride} stride, {self.padding} padding)\"\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self._bias\n",
    "    \n",
    "    @bias.setter\n",
    "    def bias(self, bias): \n",
    "        assert len(bias) == self.channels_out\n",
    "        self._bias = bias.view(self.channels_out, 1, 1)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        out_shape_width = (x.shape[-1] + (2*self.padding[1]) - self.kernel_size[1])/(self.stride[1]) + 1\n",
    "        out_shape_height = (x.shape[-2] + (2*self.padding[0]) - self.kernel_size[0])/(self.stride[0]) + 1\n",
    "        \n",
    "        assert out_shape_width.is_integer(), \"Combination of input volume, kernel_size, padding, and stride not acceptable in width dimension\"\n",
    "        assert out_shape_height.is_integer(), \"Combination of input volume, kernel_size, padding, and stride not acceptable in height dimension\"\n",
    "        \n",
    "        out_shape_width = int(out_shape_width)\n",
    "        out_shape_height = int(out_shape_height)\n",
    "        \n",
    "        input_col = self.im2col(x)\n",
    "        weights_row = self.weight.view(self.channels_out, self.kernel_size[0] * self.kernel_size[1] * self.channels_in)\n",
    "#         print(f\"input_col output {tuple(input_col.shape)}:\")\n",
    "#         print(f\"x={x}\")\n",
    "#         print(f\"input_col = {input_col.T}\")\n",
    "#         print(f\"weights_row matrix {tuple(weights_row.shape)}:\")\n",
    "#         print(f\"self.weight = {self.weight}\")\n",
    "#         print(f\"weights_row = {weights_row}\")\n",
    "        self.out = (weights_row @ input_col.T).T \n",
    "        self.out = self.out.view(batch_size, self.channels_out, out_shape_height, out_shape_width)\n",
    "        self.out = self.out + self.bias if self.bias is not None else self.out\n",
    "        return self.out\n",
    "        \n",
    "    def im2col(self, x):\n",
    "        \"\"\"\n",
    "        Our own version of im2col to make efficient conv2d.\n",
    "        Input: input tensor to unfold \n",
    "        Output: 3D tensor (N, C, L) where N is batch size, C is channels_in * self.kernel_size[0] * self.kernel_size[1], and L is how many blocks exist while sliding over a single input slice \n",
    "        \"\"\"\n",
    "        unfold = torch.nn.Unfold(self.kernel_size, 1, self.padding, self.stride)\n",
    "        output = unfold(x)\n",
    "        return output\n",
    "        \n",
    "    def parameters(self): \n",
    "        return [self.weight]\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "class Embedding: \n",
    "    def __init__(self, num_embedding, dim_embedding): \n",
    "        self.weight = torch.randn([num_embedding, dim_embedding])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Embedding({tuple(self.weight.shape)})\"\n",
    "    \n",
    "    def __call__(self, ix):\n",
    "        self.out = self.weight[ix]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "class Flatten: \n",
    "    \"\"\"\n",
    "    Flattens the input matrix outside of the batch dimension (first dimension)\n",
    "    \"\"\"\n",
    "    def __call__(self, x): \n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Flatten()\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def __call__(self, x): \n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, self.n*C)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"FlattenConsecutive({self.n})\"\n",
    "    \n",
    "# ------------------------------------------------------------------------------\n",
    "class Sequential: \n",
    "    def __init__(self, layers): \n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # return parameters for all parameters in each layer\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Sequential({self.layers})\"\n",
    "    \n",
    "    def append(self, layer): \n",
    "        self.layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  2.7375,   4.5040,   6.2706],\n",
      "          [  9.8036,  11.5702,  13.3367],\n",
      "          [ 16.8698,  18.6363,  20.4029]],\n",
      "\n",
      "         [[-30.0408, -36.4892, -42.9375],\n",
      "          [-55.8341, -62.2824, -68.7308],\n",
      "          [-81.6274, -88.0757, -94.5240]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[  2.7375,   4.5040,   6.2706],\n",
      "          [  9.8036,  11.5702,  13.3367],\n",
      "          [ 16.8698,  18.6363,  20.4029]],\n",
      "\n",
      "         [[-30.0408, -36.4891, -42.9375],\n",
      "          [-55.8341, -62.2824, -68.7308],\n",
      "          [-81.6274, -88.0757, -94.5240]]]])\n",
      "True\n",
      "torch.Size([1, 2, 3, 3])\n",
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "channels_in = 2\n",
    "channels_out = 2\n",
    "input_vol = torch.tensor(np.linspace(1,16,16),  dtype=torch.float).view(1, 1, 4, 4)\n",
    "input_vol = torch.zeros(1,channels_in,4,4)\n",
    "input_vol[0, 0,:] = input_vol_temp\n",
    "input_vol[0, 1,:] = -input_vol_temp\n",
    "kernel = torch.randn(channels_out,channels_in,2,2)\n",
    "bias = torch.randn(channels_out)\n",
    "# kernel = torch.tensor([[1.0,2.0],[3.0,4.0]], dtype=torch.float)\n",
    "# kernel = torch.tensor([[[1.0,2.0],[3.0,4.0]], [[-1.0,-2.0],[-3.0,-4.0]]], dtype=torch.float)\n",
    "kernel = kernel.view(channels_out, channels_in, 2, 2)\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "conv2d_torch = torch.nn.Conv2d(channels_in, channels_out, kernel.shape[-2:], stride=stride, padding=padding, bias=True)\n",
    "conv2d_torch.weight = torch.nn.Parameter(kernel)\n",
    "conv2d_torch.bias = torch.nn.Parameter(bias)\n",
    "conv2d_custom = Conv2d(channels_in, channels_out, kernel.shape[-2:], stride, padding, True)\n",
    "conv2d_custom.weight = kernel\n",
    "conv2d_custom.bias = bias\n",
    "\n",
    "# print(input_vol)\n",
    "# print(kernel)\n",
    "\n",
    "output_torch = conv2d_torch(input_vol)\n",
    "output_custom = conv2d_custom(input_vol)\n",
    "print(output_torch)\n",
    "print(output_custom)\n",
    "print(torch.allclose(output_torch, output_custom))\n",
    "\n",
    "print(output_torch.shape)\n",
    "print(output_custom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7291, 16, 16])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
